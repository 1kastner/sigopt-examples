{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SigOpt Explorations: How are My Hyperparameters Affecting My Training Time?\n",
    "_By [Alexandra Johnson](https://sigopt.com/about#alexandra), Software Engineer_\n",
    "\n",
    "SigOpt helps you optimize your hyperparameters, but did you know that SigOpt can also help you explore how long it takes to train your models? This jupyter notebook tutorial will show you how to take advantage of information already present in the [SigOpt API](https://sigopt.com/docs/overview) to learn new and interesting insights about how parameter values affect model training time.\n",
    "\n",
    "## How it Works\n",
    "If you've been using the SigOpt [optimization loop](https://sigopt.com/docs/overview/optimization) to tune your models, you've been requesting a suggestion, evaluating your metric, and reporting an observation. [Suggestion](https://sigopt.com/docs/objects/suggestion) and [Observation](sigopt.com/docs/objects/observation) objects expose their creation time as a unix timestamp in the `created` field. Observation created time minus suggestion created time provides an estimated training time, in seconds. This notebook walks you through fetching the observations for several similar experiments, linking each observation to its suggestion and calculating estimated training time, then plotting a graph of parameter value vs estimated training time for individual parameters. \n",
    "\n",
    "## TL/DR\n",
    "Skip to the appendix at the bottom to see how hyperparameters affect training time in our example of [Tuning a Text Classifier](blog.sigopt.com/post/133089144983/sigopt-for-ml-automatically-tuning-text).\n",
    "\n",
    "## Setup\n",
    "Run the following commands to clone this repo, install dependencies, and get this notebook up and running. After running these commands a browser window should pop up, and from there you can navigate to this notebook ([this link](http://localhost:8888/notebooks/How%20are%20My%20Hyperparameters%20Affecting%20My%20Training%20Time%3F.ipynb) should also take you directly to the notebook one jupyter is running).\n",
    "```\n",
    "git clone https://github.com/sigopt/sigopt-examples.git\n",
    "cd sigopt-examples/estimated-training-time/\n",
    "pip install -r requirements.txt\n",
    "jupyter notebook\n",
    "```\n",
    "Next, get your SigOpt API token from your [user dashboard](https://sigopt.com/user/profile). Set as the environment variable `SIGOPT_API_TOKEN`, or insert directly into the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Insert your SigOpt API token below, or set as the environment variable SIGOPT_API_TOKEN\n",
    "from sigopt.interface import Connection\n",
    "conn = Connection() # attempt to use environment variable SIGOPT_API_TOKEN\n",
    "# conn = Connection(client_token=SIGOPT_API_TOKEN) # enter token directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the ids of some interesting experiments that you'd like to look at training time for! You can find an experiment's id by clicking on the properties tab of any experiment page. This notebook can look at **ids of multiple experiments**, but they will all be **combined into one training time graph**, so pick multiple experiments only if they represent the same model. Find your experiments on your [experiment dashboard](https://sigopt.com/experiment/list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pick your experiment ids! Use multiple ids if you used multiple experiments to train the same model\n",
    "# experiment_ids = [] \n",
    "experiments = [conn.experiments(id).fetch() for id in experiment_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Want to ensure we're looking at experiments that use the same parameters\n",
    "def equal(x, y):\n",
    "    assert x == y\n",
    "    return x\n",
    "parameter_names = reduce(equal, [sorted(p.name for p in e.parameters) for e in experiments])\n",
    "print parameter_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Total number of observations we'll be considering\n",
    "reduce(lambda x, y: x + y, [e.progress.observation_count for e in experiments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetch all observations and all suggestions for all experiments\n",
    "observations = []\n",
    "suggestions = []\n",
    "for e in experiments:\n",
    "    obs = conn.experiments(e.id).observations().fetch()\n",
    "    observations.extend(obs.data)\n",
    "    suggs = conn.experiments(e.id).suggestions().fetch()\n",
    "    suggestions.extend(suggs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a map of observation id -> estimated training time\n",
    "estimated_training_times_by_id = {}\n",
    "suggestions_by_id = {s.id: s for s in suggestions}\n",
    "for o in observations:\n",
    "    if o.suggestion:\n",
    "        suggestion = suggestions_by_id.get(o.suggestion)\n",
    "        if suggestion:\n",
    "            estimated_training_times_by_id[o.id] = (o.created - suggestion.created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create x,y axes for our graphs, one graph per parameter\n",
    "# y axis is always estimated training times\n",
    "# x axis is parameter values\n",
    "# Don't include observations for which there is no estimated training time (ie, entered assignments manually)\n",
    "# rather than using a suggestion\n",
    "estimated_training_times = []\n",
    "parameter_values = {param: [] for param in parameter_names}\n",
    "for o in observations:\n",
    "    ett = estimated_training_times_by_id.get(o.id, None)\n",
    "    if ett is not None:\n",
    "        estimated_training_times.append(ett)\n",
    "        for (param, value) in o.assignments.to_json().iteritems():\n",
    "            parameter_values[param].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity check: ensure that all lists have the same length\n",
    "count_observations_with_training_time = len(estimated_training_times)\n",
    "for (_, values) in parameter_values.iteritems():\n",
    "    assert len(values) == count_observations_with_training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot training time versus parameter value for each parameter\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from plotly.graph_objs import Scatter\n",
    "\n",
    "init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "for (param, values) in parameter_values.iteritems():\n",
    "    iplot({\n",
    "            'data': [Scatter(x=values, y=estimated_training_times, mode=\"markers\")],\n",
    "            'layout': {'title': 'Estimated Training Time (s) vs {}'.format(param)}\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Did you notice anything interesting with your experiments? Do the graphs match up to your intuition about how parameter values affect training time? Email <contact@sigopt.com> or tweet to [@SigOpt](twitter.com/sigopt) to let us know what you find! Happy Optimizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Advanced Users\n",
    "Advanced users can take advtange of [metadata](https://sigopt.com/docs/overview/metadata) to store their own information about training time.\n",
    "\n",
    "Metadata is a user-provided object that SigOpt stores on your behalf under the metadata field. Think of metadata as your annotation for a SigOpt object. This field is currently supported by [Experiments](https://sigopt.com/docs/endpoints/experiments), [Observations](https://sigopt.com/docs/endpoints/observations) and [Suggestions](https://sigopt.com/docs/endpoints/suggestions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example evaluation loop using metadata to store total training time\n",
    "# Make sure you've instantiated conn from earlier in the notebook\n",
    "from time import time\n",
    "# experiment_id = USER DEFINED\n",
    "\n",
    "for _ in range(60):\n",
    "    suggestion = conn.experiments(experiment_id).suggestions().create()\n",
    "    start_ts = time()\n",
    "    value = evaluate_metric(suggestion.assignments) # You implement this\n",
    "    end_ts = time()\n",
    "    # Report an observation with training_time metadata\n",
    "    observation = conn.experiments(experiment_id).observations().create(\n",
    "        suggestion=suggestion.id,\n",
    "        value=value,\n",
    "        metadata={'training_time': (end_ts - start_ts)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: What we noticed\n",
    "We ran this notebook on three experiments from our text classifier example, and produced the graphs below. It's interesting to note how by visual inspection, it appears that the two parameters with the most influence on time are `min_n_gram` and `n_gram_offset`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Estimated training time versus log regularization coefficient](https://github.com/sigopt/sigopt-examples/blob/master/estimated-training-time/ett-vs-log-reg-coefficient.png?raw=true)\n",
    "![Estimated training time versus l1 coefficient](https://github.com/sigopt/sigopt-examples/blob/master/estimated-training-time/ett-vs-l1-coefficient.png?raw=true)\n",
    "![Estimated training time versus log minimum document frequency](https://github.com/sigopt/sigopt-examples/blob/master/estimated-training-time/ett-vs-log-min-df.png?raw=true)\n",
    "![Estimated training time versus document frequency offset](https://github.com/sigopt/sigopt-examples/blob/master/estimated-training-time/ett-vs-df-offset.png?raw=true)\n",
    "![Estimated training time versus min n-gram](https://github.com/sigopt/sigopt-examples/blob/master/estimated-training-time/ett-vs-min-n-gram.png?raw=true)\n",
    "![Estimated training time versus n-gram offset](https://github.com/sigopt/sigopt-examples/blob/master/estimated-training-time/ett-vs-n-gram-offset.png?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
